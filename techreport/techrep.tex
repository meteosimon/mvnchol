\documentclass{article}
\usepackage{fullpage}
%\linespread{1.25}
%\usepackage{xcolor}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{hyperref}
%\usepackage[round]{natbib}
%\renewcommand{\familydefault}{\sfdefault}
%\setlength{\parindent}{0pt}
%\setlength{\parskip}{10pt plus4pt minus4pt}

\begin{document}

\title{Derivatives of the Multivariate Gaussian Distribution}
\author{}
\maketitle

\section{Full Parameterization}

The likelihood $L$ of the multivariate Gaussian or normal distribution~(MVN) with $k$ dimensions for a single $k$ dimensional observation $y = (y_1, y_2, \dots, y_k)^\top$ is expressed by
%
\begin{equation}
 L(y|\mu, \Sigma) = (2\pi)^{-\frac{k}{2}} |\Sigma|^{-\frac{1}{2}} \exp \left(-\frac{1}{2} (y-\mu)^\top \Sigma^{-1} (y-\mu) \right),
\end{equation}
%
where $\mu = (\mu_1, \mu_2, \dots, \mu_k)^\top$ denotes the vector of the mean parameters and $\Sigma$ denotes the covariance matrix. Thus the log-likelihood $l=\log(L)$ is
%
\begin{equation}
 l(\mu, \Sigma|y) = -\frac{k}{2}\log(2\pi) - \frac{1}{2}\log(|\Sigma|) - \frac{1}{2} (y-\mu)^\top \Sigma^{-1} (y-\mu).
 \label{eq:ll1}
\end{equation}
%
$\Sigma$ can be decomposed into
%
\begin{equation}
 \Sigma = D \Omega D, \qquad \text{thus} \qquad \Sigma^{-1} = D^{-1} \Omega^{-1} D^{-1}, \qquad \text{and} \qquad |\Sigma|=|D| |\Omega| |D|
\end{equation}
%
with
%
\begin{equation}
 D=
  \begin{pmatrix}
   \sigma_1 & 0 & \cdots & 0 \\
   0 & \sigma_2 & \cdots & 0 \\
   \vdots  & \vdots  & \ddots & \vdots  \\
   0 & 0 & \cdots & \sigma_k
  \end{pmatrix},
  \qquad \text{and} \qquad
 \Omega=
  \begin{pmatrix}
   1 & \rho_{12} & \cdots & \rho_{1k} \\
   \rho_{12} & 1 & \cdots & \rho_{2k} \\
   \vdots & \vdots & \ddots & \vdots \\
   \rho_{1k} & \rho_{2k} & \cdots & 1
  \end{pmatrix},
  \label{eq:mat}
\end{equation}
%
where $\sigma_i$ and $\rho_{ij}$ denote the standard deviation and the correlation, respectively. Thus, the log-likelihood can be rewritten as,
%
\begin{equation}
 l(\mu, D, \Omega|y) = -\frac{k}{2}\log(2\pi) - \log(|D|) - \frac{1}{2}\log(|\Omega|) - \frac{1}{2} (y-\mu)^\top D^{-1} \Omega^{-1} D^{-1} (y-\mu).
\end{equation}
%
$|D| = \prod_{i=1}^{k} \sigma_i$ and scaling $\tilde{y_i} = (y_i - \mu_i) / \sigma_i$ yields
%
\begin{equation}
 l = -\frac{k}{2}\log(2\pi) - \sum_{i=1}^{k} \log(\sigma_i) - \frac{1}{2}\log(|\Omega|) - \frac{1}{2} \tilde{y}^\top \Omega^{-1} \tilde{y}.
 \label{eq:ll2}
\end{equation}
%
In the following the elements of $\Sigma^{-1}$ and $\Omega^{-1}$ will be denoted by $\varsigma_{ij}$ and $\omega_{ij}$, respectively.

Deriving Eq.~\ref{eq:ll1} (using formula 86 in the
%\href{http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf}
{matrix cookbook}) yields
%
\begin{equation}
 \frac{\partial l}{\partial \mu} = \Sigma^{-1} (y-\mu), \qquad \text{or} \qquad
 \frac{\partial l}{\partial \mu_i} = \sum_{j=1}^{k} \varsigma_{ij} (y_j - \mu_j).
 \label{eq:mu}
\end{equation}
%
As
%
\begin{equation}
 \frac{\partial}{\partial \tilde{y}} \tilde{y}^\top \Omega^{-1} \tilde{y} = 2\Omega^{-1}\tilde{y}, \qquad \text{and thus} \qquad
 \frac{\partial}{\partial \tilde{y}_i} \tilde{y}^\top \Omega^{-1} \tilde{y} = 2\sum_{j=1}^{k} \omega_{ij} \tilde{y}_j,
\end{equation}
%
and
%
\begin{equation}
 \frac{\partial \tilde{y}_i}{\partial \sigma_i} = - \frac{1}{\sigma_i} \tilde{y}_i, \qquad \text \qquad
 \frac{\partial l}{\partial \sigma_i} = \frac{\partial l}{\partial \tilde{y}_i} \frac{\partial \tilde{y}_i}{\partial \sigma_i}
\end{equation}
%
\
deriving Eq.~\ref{eq:ll2} with respect to $\sigma_i$ yields
%
\begin{equation}
 \frac{\partial l}{\partial \sigma_i} = -\frac{1}{\sigma_i} + \frac{1}{\sigma_i} \tilde{y}_i \sum_{j=1}^{k} \omega_{ij} \tilde{y}_j.
 \label{eq:sig}
\end{equation}
%
Applying formula 57 and 61 from the matrix cookbook yields
%
\begin{equation}
 \frac{\partial l}{\partial \Omega} = -\frac{1}{2} \Omega^{-1} + \frac{1}{2} \Omega^{-1} \tilde{y} \tilde{y}^\top \Omega^{-1},
\end{equation}
%
or
%
\begin{equation}
 \frac{\partial l}{\partial \rho_{ij}} = -\frac{1}{2} \omega_{ij} + \frac{1}{2} \left( \sum_{m=1}^{k} \omega_{im}\tilde{y}_m \right) \left( \sum_{m=1}^{k} \omega_{jm}\tilde{y}_m \right).
 \label{eq:rho}
\end{equation}
%
Finally, Eq.~\ref{eq:mu}, \ref{eq:sig} and \ref{eq:rho} contain all derivatives.

In the following we skip the indices. Now link functions between $\sigma$ (log) and $\rho$ (rhogit) and its predictors $\eta_{\sigma}$ and $\eta_{\rho}$ are introduced by
%
\begin{equation}
 \log(\sigma) = \eta_{\sigma}, \qquad \sigma = \exp( \eta_{\sigma} ),
\end{equation}
%
and
%
\begin{equation}
 \frac{\rho}{\sqrt{1-\rho^2}} = \eta_{\rho}, \qquad \rho = \frac{\eta}{\sqrt{1+\eta_{\rho}^2}},
\end{equation} 
%
respectively. With its derivatives
%
\begin{equation}
 \frac{\partial \sigma}{\partial \eta_{\sigma}} = \exp(\eta_{\sigma}) = \sigma,
\end{equation}
%
and
%
\begin{equation}
 \frac{\partial \rho}{\partial \eta_{\rho}} = \frac{1}{(1+\eta_{\rho}^2)^\frac{3}{2}}.
\end{equation}
%

\newpage
\section{The AR(1)-Process Parameterization}

We assume that the response variables $y_1, y_2, \dots, y_k$ are samples from an order~1 autoregressive process. Thus, $\Omega$ in Eq.~\ref{eq:mat} can be re-written as,
%
\begin{equation}
 \Omega=
  \begin{pmatrix}
   1 & \rho & \rho^2 & \cdots & \rho^{k-1} \\
   \rho & 1 & \rho & \cdots & \rho^{k-2} \\
   \rho^2 & \rho & 1 & \cdots & \rho^{k-3} \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
   \rho^{k-1} & \rho^{k-2} & \rho^{k-3} & \cdots & 1
  \end{pmatrix},
\end{equation}
%
with
\begin{equation}
 \Omega^{-1}= \frac{1}{1-\rho^2}
  \begin{pmatrix}
   1 & -\rho & 0 & \cdots & 0 \\
   -\rho & 1+\rho^2 & -\rho & \cdots & 0 \\
   0 & -\rho & 1+\rho^2 & \cdots & 0 \\
   \vdots & \vdots & \vdots & \ddots & \vdots \\
   0 & 0 & 0 & \cdots & 1
  \end{pmatrix},
\end{equation}
%
Now,
\begin{equation}
  |\Omega| = (1-\rho^2)^{k-1}
  \qquad \text{and} \qquad
  \tilde{y}^\top \Omega^{-1} \tilde{y} = \frac{1}{1-\rho^2} \left( \sum_{i=1}^{k} \tilde{y}_i^2 - 2\rho \sum_{i=1}^{k-1} \tilde{y}_i \tilde{y}_{i+1} + \rho^2 \sum_{i=2}^{k-1} \tilde{y}_i^2 \right).
\end{equation}
%
The log-likelihood Eq.~\ref{eq:ll2} can now be rewritten as,
%
\begin{equation}
 l = -\frac{k}{2}\log(2\pi) - \sum_{i=1}^{k} \log(\sigma_i) - \frac{k-1}{2}\log(1-\rho^2) - \frac{1}{2(1-\rho^2)} \left( \sum_{i=1}^{k} \tilde{y}_i^2 - 2 \rho \sum_{i=1}^{k-1} \tilde{y}_i \tilde{y}_{i+1} + \rho^2 \sum_{i=2}^{k-1} \tilde{y}_i^2 \right).
 \label{eq:llAR}
\end{equation}
%
The derivitates are now,
%
\begin{equation}
  \frac{\partial l}{\partial \mu_i} = \frac{1}{\sigma_i(1-\rho^2)}
  \left( \tilde{y}_i
         - \underbrace{\rho\tilde{y}_{i-1}}_{\text{if} \: i \neq 1}
         - \underbrace{\rho\tilde{y}_{i+1}}_{\text{if} \: i \neq k}
         + \underbrace{\rho^2\tilde{y}_i}_{\text{if} \: 1 < i < k}
  \right),
\end{equation}
%
and
%
\begin{equation}
  \frac{\partial l}{\partial \sigma_i} = - \frac{1}{\sigma_i} + \frac{\tilde{y}_i}{\sigma_i(1-\rho^2)}
  \left( \tilde{y}_i
         - \underbrace{\rho\tilde{y}_{i-1}}_{\text{if} \: i \neq 1}
         - \underbrace{\rho\tilde{y}_{i+1}}_{\text{if} \: i \neq k}
         + \underbrace{\rho^2\tilde{y}_i}_{\text{if} \: 1 < i < k}
  \right),
\end{equation}
%
and
%
\begin{equation}
  \frac{\partial l}{\partial \rho}
  = \frac{(k-1)\rho}{1-\rho^2}
  + \frac{1}{1-\rho^2} \left( \sum_{i=1}^{k-1} \tilde{y}_i \tilde{y}_{i+1} - \rho \sum_{i=2}^{k-1} \tilde{y}_i^2  \right)
  - \frac{\rho}{(1-\rho^2)^2} \left( \sum_{i=1}^{k} \tilde{y}_i^2 - 2\rho \sum_{i=1}^{k-1} \tilde{y}_i \tilde{y}_{i+1} + \rho^2 \sum_{i=2}^{k-1} \tilde{y}_i^2 \right).
\end{equation}


\newpage
\section{Cholesky Parametrisation}

Any symmetric positive definite matrix can be decomposed as the product of a
lower triangular matrix $L$ and its transpose $L^\top$. In this manner 
we can write the covariance matrix $\Sigma$ as
%
\begin{equation}
 \Sigma = L L^\top
 \qquad \text{,} \qquad
 \Sigma^{-1} = (L^{-1})^\top L^{-1}.
\end{equation}
%
Equation \ref{eq:ll1} for the log-likelihood can be rewritten in terms of $L$ as 
%
\begin{equation}
 l(\mu, L|y) = -\frac{k}{2}\log(2\pi) - \frac{1}{2}\log(|L L^\top|) - \frac{1}{2} (y-\mu)^\top 
	(L L^\top)^{-1} (y-\mu).
 \label{eq:ll_chol}
\end{equation}
%
Since $|A B| = |A| |B|$, $|A^n| = |A|^n$ and $|A^\top| = |A|$, the 
log-likelihood can be formulated in terms of only $L^{-1}$ instead of 
both $L$ and $L^{-1}$:
%
\begin{equation}
	l(\mu, L^{-1}|y) = -\frac{k}{2}\log(2\pi) + \log(|L^{-1}|) 
	- \frac{1}{2} (y-\mu)^\top (L^{-1})^\top L^{-1} (y-\mu).
 \label{eq:ll_choles}
\end{equation}
%

The derivative of Eq. \ref{eq:ll_choles} with respect to $\mu$ is the same 
as in Eq. \ref{eq:mu}, with $\Sigma^{-1} =  (L^{-1})^\top L^{-1}$:
%
\begin{equation}
 \frac{\partial l}{\partial \mu} = (L^{-1})^\top L^{-1} (y-\mu).
 \label{eq:mu_chol}
\end{equation}
%
The partial derivatives of the log-likelihood with respect to elements of 
$L^{-1}$ require a little more computation. Since $L$ is lower triangular, 
$L^{-1}$ is upper triangular. We define the nontrivial entries of $L^{-1}$ 
to be $\lambda_{ij}$ with $1 \leq i \leq j \leq k$: 
%
\begin{equation}
  \L^{-1}=
  \begin{pmatrix}
    \lambda_{11} & \lambda_{12} & \lambda_{13} & \cdots & \lambda_{1k} \\
    0 & \lambda_{22} & \lambda_{23} & \cdots & \lambda_{2k} \\
    0 & 0 & \lambda_{33} & \cdots & \lambda_{3k} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & \lambda_{kk}
  \end{pmatrix}.
\end{equation}
%
Furthermore, let $\tilde{y} = y - \mu$ with elements $\tilde{y}_i$ for 
$i = 1, 2, \dots, k$. 
Of the three terms in Eq. \ref{eq:ll_choles}, the first is constant and the 
second simplifies to $\sum_{i=1}^k \log{\lambda_{ii}}$, i.e.\ depends only 
on the diagonal elements of $L^{-1}$. Let us investigate the third term, which 
is equivalent to $-1/2 z^\top z$, where z is given by:
%
\begin{equation}
	z = \L^{-1} \tilde{y}=
  \begin{pmatrix}
    \lambda_{11} & \lambda_{12} & \lambda_{13} & \cdots & \lambda_{1k} \\
    0 & \lambda_{22} & \lambda_{23} & \cdots & \lambda_{2k} \\
    0 & 0 & \lambda_{33} & \cdots & \lambda_{3k} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & \lambda_{kk}
  \end{pmatrix}
  \begin{pmatrix}
    \tilde{y}_1 \\
    \tilde{y}_2 \\
    \tilde{y}_3	\\
    \vdots \\
    \tilde{y}_k
  \end{pmatrix}.
\end{equation}
%
We see that 
%
\begin{equation}
	z =
  \begin{pmatrix}
    \sum_{j=1}^k (\tilde{y}_j \lambda_{1j}) \\
    \sum_{j=2}^k (\tilde{y}_j \lambda_{2j}) \\
    \sum_{j=3}^k (\tilde{y}_j \lambda_{3j}) \\
    \vdots \\
    \sum_{j=k}^k (\tilde{y}_j \lambda_{kj}) 
  \end{pmatrix}
  \qquad \text{and} \qquad
  z^\top z =
	\sum_{i=1}^k \left( \sum_{j=i}^k \left( \tilde{y}_j \lambda_{ij} \right) \right)^2.
\end{equation}
%
Now we can rewrite the log-likelihood (Eq. \ref{eq:ll_choles}) in terms of the 
elements, $\lambda_{ij}$, of $L^{-1}$ to calculate the partial derivatives:
%
\begin{equation}
	l(\mu, L^{-1}|y) = -\frac{k}{2}\log(2\pi) + \sum_{i=1}^k 
	\log{\lambda_{ii}} - \frac{1}{2} \sum_{i=1}^k 
	\left( \sum_{j=i}^k \left( \tilde{y}_j \lambda_{ij} 
	\right) \right)^2.
 \label{eq:ll_chol_terms}
\end{equation}
%
For the case where $m \neq n$, 
%
\begin{equation}
  \frac{\partial l}{\partial \lambda_{mn}} = 
  -\frac{1}{2} \sum_{i=1}^k \left[2 \left(\sum_{j=i}^k (\tilde{y}_j \lambda_{ij}) 
	\right) \tilde{y}_n \mathbbm{1}_m(i) \right] = 
  - \tilde{y}_n \sum_{j = m}^{k} \left( \tilde{y}_j \lambda_{mj} \right).	
\end{equation}
%
If $m = n$, the second term in the log-likelihood also has nonzero partial 
derivative with respect to $\lambda_{mn}$ and
%
\begin{equation}
  \frac{\partial l}{\partial \lambda_{mm}} = 
	\frac{1}{\lambda_{mm}}- \tilde{y}_m \sum_{j = m}^{k} 
	\left( \tilde{y}_j \lambda_{mj} \right).	
\end{equation}.

Since all entries on the main diagonal of $L^{-1}$ must be positive 
(i.e.\ $\lambda_{mm} > 0$), we use a log link function. For the 
off diagonal entries we use an identity link. Then
%
\begin{equation}
	\log(\lambda_{mm}) = \eta_{\lambda,mm}, \quad
        \frac{\partial l}{\partial \eta_{\lambda,mm}} =
	\frac{\partial l}{\partial \lambda_{mm}} \lambda_{mm}
	\qquad \text{and} \qquad  
        \lambda_{mn} = \eta_{\lambda,mn},
\end{equation}
%
resulting in 
%
\begin{equation}
  \frac{\partial l}{\partial \eta_{\lambda,mm}} =
  1 - \lambda_{mm} \tilde{y}_m \sum_{j=m}^k (\tilde{y}_j \lambda_{mj})
  \qquad \text{and} \qquad
  \frac{\partial l}{\partial \eta_{\lambda,mn}} = 
	- \tilde{y}_n \sum_{j = m}^{k} \left( \tilde{y}_j \lambda_{mj} \right).
\end{equation}
%
For the $\mu$ vector an identity link, 
$\mu_i = \eta_{\mu,i}$, also suffices and Eq. \ref{eq:mu_chol} becomes
%
\begin{equation}
  \frac{\partial l}{\partial \eta_{\mu, i}} = 
	\sum_{j=1}^k \varsigma_{ij} \tilde{y}_j, 
\end{equation}
%
where $\varsigma_{ij}$ refers to the corresponding element of 
$\Sigma^{-1} = (L^{-1})^\top L^{-1}$.

\end{document}

